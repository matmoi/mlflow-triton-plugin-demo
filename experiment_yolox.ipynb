{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOLOX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "from io import BytesIO\n",
    "import cv2\n",
    "import urllib\n",
    "import numpy as np\n",
    "import onnxruntime\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import tritonclient.grpc as grpcclient\n",
    "import os\n",
    "\n",
    "TRITON_PORT_GRPC = os.getenv(\"TRITON_PORT_GRPC\", \"8001\")\n",
    "MLFLOW_PORT = os.getenv(\"MLFLOW_PORT\", \"8080\")\n",
    "\n",
    "def preprocess(img, input_size=(640,640), swap=(2, 0, 1)):\n",
    "    if len(img.shape) == 3:\n",
    "        padded_img = np.ones((input_size[0], input_size[1], 3), dtype=np.uint8) * 114\n",
    "    else:\n",
    "        padded_img = np.ones(input_size, dtype=np.uint8) * 114\n",
    "\n",
    "    r = min(input_size[0] / img.shape[0], input_size[1] / img.shape[1])\n",
    "    resized_img = cv2.resize(\n",
    "        img,\n",
    "        (int(img.shape[1] * r), int(img.shape[0] * r)),\n",
    "        interpolation=cv2.INTER_LINEAR,\n",
    "    ).astype(np.uint8)\n",
    "    padded_img[: int(img.shape[0] * r), : int(img.shape[1] * r)] = resized_img\n",
    "\n",
    "    padded_img = padded_img.transpose(swap)\n",
    "    padded_img = np.ascontiguousarray(padded_img, dtype=np.float32)\n",
    "    return padded_img, r\n",
    "\n",
    "def postprocess(outputs, input_size=(640,640), p6=False):\n",
    "    grids = []\n",
    "    expanded_strides = []\n",
    "    strides = [8, 16, 32] if not p6 else [8, 16, 32, 64]\n",
    "\n",
    "    hsizes = [input_size[0] // stride for stride in strides]\n",
    "    wsizes = [input_size[1] // stride for stride in strides]\n",
    "\n",
    "    for hsize, wsize, stride in zip(hsizes, wsizes, strides):\n",
    "        xv, yv = np.meshgrid(np.arange(wsize), np.arange(hsize))\n",
    "        grid = np.stack((xv, yv), 2).reshape(1, -1, 2)\n",
    "        grids.append(grid)\n",
    "        shape = grid.shape[:2]\n",
    "        expanded_strides.append(np.full((*shape, 1), stride))\n",
    "\n",
    "    grids = np.concatenate(grids, 1)\n",
    "    expanded_strides = np.concatenate(expanded_strides, 1)\n",
    "    outputs[..., :2] = (outputs[..., :2] + grids) * expanded_strides\n",
    "    outputs[..., 2:4] = np.exp(outputs[..., 2:4]) * expanded_strides\n",
    "\n",
    "    return outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load YOLOX model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ONNX_MODEL_NAME = \"yolox_s\"\n",
    "\n",
    "req = urllib.request.urlopen(f\"https://github.com/Megvii-BaseDetection/YOLOX/releases/download/0.1.1rc0/{ONNX_MODEL_NAME}.onnx\")\n",
    "model_data = BytesIO(req.read())\n",
    "model = onnx.load(model_data)\n",
    "onnx.checker.check_model(model)\n",
    "# onnx.save_model(model, \"yolox_s_save.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Register model to mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow.models import infer_signature\n",
    "\n",
    "mlflow.set_tracking_uri(f\"http://localhost:{MLFLOW_PORT}\")\n",
    "\n",
    "mlflow.set_experiment(ONNX_MODEL_NAME)\n",
    "\n",
    "with mlflow.start_run():\n",
    "    mlflow_model = mlflow.onnx.log_model(model, ONNX_MODEL_NAME, registered_model_name=ONNX_MODEL_NAME, save_as_external_data=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy model to Triton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow import MlflowClient\n",
    "from mlflow.deployments import get_deploy_client\n",
    "\n",
    "# retrieve the most recent model version\n",
    "mlflow_client = MlflowClient()\n",
    "model_version = max(m.version for m in mlflow_client.search_model_versions(f\"name='{ONNX_MODEL_NAME}'\"))\n",
    "\n",
    "client = get_deploy_client('triton')\n",
    "client.create_deployment(ONNX_MODEL_NAME, f\"models:/{ONNX_MODEL_NAME}/{model_version}\", flavor=\"onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess image\n",
    "IMAGE_URL = \"https://github.com/ultralytics/ultralytics/blob/main/ultralytics/assets/bus.jpg?raw=true\"\n",
    "req = urllib.request.urlopen(IMAGE_URL)\n",
    "origin_img = np.asarray(Image.open(BytesIO(req.read()))).astype(np.uint8)\n",
    "img, ratio = preprocess(origin_img[:,:,:3])\n",
    "\n",
    "plt.axis('off')\n",
    "plt.imshow(origin_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COCO_CLASSES=(\"person\",\"bicycle\",\"car\",\"motorcycle\",\"airplane\",\"bus\",\"train\",\"truck\",\"boat\",\"trafficlight\",\"firehydrant\",\"stopsign\",\"parkingmeter\",\"bench\",\"bird\",\"cat\",\"dog\",\"horse\",\"sheep\",\"cow\",\"elephant\",\"bear\",\"zebra\",\"giraffe\",\"backpack\",\"umbrella\",\"handbag\",\"tie\",\"suitcase\",\"frisbee\",\"skis\",\"snowboard\",\"sportsball\",\"kite\",\"baseballbat\",\"baseballglove\",\"skateboard\",\"surfboard\",\"tennisracket\",\"bottle\",\"wineglass\",\"cup\",\"fork\",\"knife\",\"spoon\",\"bowl\",\"banana\",\"apple\",\"sandwich\",\"orange\",\"broccoli\",\"carrot\",\"hotdog\",\"pizza\",\"donut\",\"cake\",\"chair\",\"couch\",\"pottedplant\",\"bed\",\"diningtable\",\"toilet\",\"tv\",\"laptop\",\"mouse\",\"remote\",\"keyboard\",\"cellphone\",\"microwave\",\"oven\",\"toaster\",\"sink\",\"refrigerator\",\"book\",\"clock\",\"vase\",\"scissors\",\"teddybear\",\"hairdrier\",\"toothbrush\",)\n",
    "COLORS=np.array([0.000,0.447,0.741,0.850,0.325,0.098,0.929,0.694,0.125,0.494,0.184,0.556,0.466,0.674,0.188,0.301,0.745,0.933,0.635,0.078,0.184,0.300,0.300,0.300,0.600,0.600,0.600,1.000,0.000,0.000,1.000,0.500,0.000,0.749,0.749,0.000,0.000,1.000,0.000,0.000,0.000,1.000,0.667,0.000,1.000,0.333,0.333,0.000,0.333,0.667,0.000,0.333,1.000,0.000,0.667,0.333,0.000,0.667,0.667,0.000,0.667,1.000,0.000,1.000,0.333,0.000,1.000,0.667,0.000,1.000,1.000,0.000,0.000,0.333,0.500,0.000,0.667,0.500,0.000,1.000,0.500,0.333,0.000,0.500,0.333,0.333,0.500,0.333,0.667,0.500,0.333,1.000,0.500,0.667,0.000,0.500,0.667,0.333,0.500,0.667,0.667,0.500,0.667,1.000,0.500,1.000,0.000,0.500,1.000,0.333,0.500,1.000,0.667,0.500,1.000,1.000,0.500,0.000,0.333,1.000,0.000,0.667,1.000,0.000,1.000,1.000,0.333,0.000,1.000,0.333,0.333,1.000,0.333,0.667,1.000,0.333,1.000,1.000,0.667,0.000,1.000,0.667,0.333,1.000,0.667,0.667,1.000,0.667,1.000,1.000,1.000,0.000,1.000,1.000,0.333,1.000,1.000,0.667,1.000,0.333,0.000,0.000,0.500,0.000,0.000,0.667,0.000,0.000,0.833,0.000,0.000,1.000,0.000,0.000,0.000,0.167,0.000,0.000,0.333,0.000,0.000,0.500,0.000,0.000,0.667,0.000,0.000,0.833,0.000,0.000,1.000,0.000,0.000,0.000,0.167,0.000,0.000,0.333,0.000,0.000,0.500,0.000,0.000,0.667,0.000,0.000,0.833,0.000,0.000,1.000,0.000,0.000,0.000,0.143,0.143,0.143,0.286,0.286,0.286,0.429,0.429,0.429,0.571,0.571,0.571,0.714,0.714,0.714,0.857,0.857,0.857,0.000,0.447,0.741,0.314,0.717,0.741,0.50,0.5,0]).astype(np.float32).reshape(-1,3)\n",
    "\n",
    "def multiclass_nms_class_agnostic(boxes, scores, nms_thr, score_thr):\n",
    "    \"\"\"Multiclass NMS implemented in Numpy. Class-agnostic version.\"\"\"\n",
    "    cls_inds = scores.argmax(1)\n",
    "    cls_scores = scores[np.arange(len(cls_inds)), cls_inds]\n",
    "\n",
    "    valid_score_mask = cls_scores > score_thr\n",
    "    if valid_score_mask.sum() == 0:\n",
    "        return None\n",
    "    valid_scores = cls_scores[valid_score_mask]\n",
    "    valid_boxes = boxes[valid_score_mask]\n",
    "    valid_cls_inds = cls_inds[valid_score_mask]\n",
    "    keep = nms(valid_boxes, valid_scores, nms_thr)\n",
    "    if keep:\n",
    "        dets = np.concatenate(\n",
    "            [valid_boxes[keep], valid_scores[keep, None], valid_cls_inds[keep, None]], 1\n",
    "        )\n",
    "    return dets\n",
    "\n",
    "def nms(boxes, scores, nms_thr):\n",
    "    \"\"\"Single class NMS implemented in Numpy.\"\"\"\n",
    "    x1 = boxes[:, 0]\n",
    "    y1 = boxes[:, 1]\n",
    "    x2 = boxes[:, 2]\n",
    "    y2 = boxes[:, 3]\n",
    "\n",
    "    areas = (x2 - x1 + 1) * (y2 - y1 + 1)\n",
    "    order = scores.argsort()[::-1]\n",
    "\n",
    "    keep = []\n",
    "    while order.size > 0:\n",
    "        i = order[0]\n",
    "        keep.append(i)\n",
    "        xx1 = np.maximum(x1[i], x1[order[1:]])\n",
    "        yy1 = np.maximum(y1[i], y1[order[1:]])\n",
    "        xx2 = np.minimum(x2[i], x2[order[1:]])\n",
    "        yy2 = np.minimum(y2[i], y2[order[1:]])\n",
    "\n",
    "        w = np.maximum(0.0, xx2 - xx1 + 1)\n",
    "        h = np.maximum(0.0, yy2 - yy1 + 1)\n",
    "        inter = w * h\n",
    "        ovr = inter / (areas[i] + areas[order[1:]] - inter)\n",
    "\n",
    "        inds = np.where(ovr <= nms_thr)[0]\n",
    "        order = order[inds + 1]\n",
    "\n",
    "    return keep\n",
    "\n",
    "\n",
    "def visualize(img, boxes, scores, cls_ids, conf=0.5, class_names=None):\n",
    "\n",
    "    for i in range(len(boxes)):\n",
    "        box = boxes[i]\n",
    "        cls_id = int(cls_ids[i])\n",
    "        score = scores[i]\n",
    "        if score < conf:\n",
    "            continue\n",
    "        x0 = int(box[0])\n",
    "        y0 = int(box[1])\n",
    "        x1 = int(box[2])\n",
    "        y1 = int(box[3])\n",
    "\n",
    "        color = (COLORS[cls_id] * 255).astype(np.uint8).tolist()\n",
    "        text = '{}:{:.1f}%'.format(class_names[cls_id], score * 100)\n",
    "        txt_color = (0, 0, 0) if np.mean(COLORS[cls_id]) > 0.5 else (255, 255, 255)\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "\n",
    "        txt_size = cv2.getTextSize(text, font, 0.4, 1)[0]\n",
    "        cv2.rectangle(img, (x0, y0), (x1, y1), color, 2)\n",
    "\n",
    "        txt_bk_color = (COLORS[cls_id] * 255 * 0.7).astype(np.uint8).tolist()\n",
    "        cv2.rectangle(\n",
    "            img,\n",
    "            (x0, y0 + 1),\n",
    "            (x0 + txt_size[0] + 1, y0 + int(1.5*txt_size[1])),\n",
    "            txt_bk_color,\n",
    "            -1\n",
    "        )\n",
    "        cv2.putText(img, text, (x0, y0 + txt_size[1]), font, 0.4, txt_color, thickness=1)\n",
    "\n",
    "    return img\n",
    "\n",
    "\n",
    "# create triton client\n",
    "#  Note that the same model can be executed locally, using the following code snippet:\n",
    "# \n",
    "# session = onnxruntime.InferenceSession(model.SerializeToString())\n",
    "# ort_inputs = {session.get_inputs()[0].name: img[None, :, :, :]}\n",
    "# output = session.run(None, ort_inputs)[0]\n",
    "\n",
    "triton_client = grpcclient.InferenceServerClient(\n",
    "    url=f\"localhost:{TRITON_PORT_GRPC}\", verbose=True\n",
    ")\n",
    "input = grpcclient.InferInput(\"images\", [1,3,640,640], \"FP32\")\n",
    "input.set_data_from_numpy(img[None,:,:,:])\n",
    "result = triton_client.infer(ONNX_MODEL_NAME, inputs=[input])\n",
    "output = np.copy(result.as_numpy(\"output\"))\n",
    "\n",
    "# postprocessing step - to extract bounding boxes\n",
    "\n",
    "predictions = postprocess(output)[0]\n",
    "boxes = predictions[:, :4]\n",
    "scores = predictions[:, 4:5] * predictions[:, 5:]\n",
    "\n",
    "boxes_xyxy = np.ones_like(boxes)\n",
    "boxes_xyxy[:, 0] = boxes[:, 0] - boxes[:, 2]/2.\n",
    "boxes_xyxy[:, 1] = boxes[:, 1] - boxes[:, 3]/2.\n",
    "boxes_xyxy[:, 2] = boxes[:, 0] + boxes[:, 2]/2.\n",
    "boxes_xyxy[:, 3] = boxes[:, 1] + boxes[:, 3]/2.\n",
    "boxes_xyxy /= ratio\n",
    "\n",
    "dets = multiclass_nms_class_agnostic(boxes_xyxy, scores, nms_thr=0.45, score_thr=0.1)\n",
    "if dets is not None:\n",
    "    final_boxes, final_scores, final_cls_inds = dets[:, :4], dets[:, 4], dets[:, 5]\n",
    "    origin_img = visualize(origin_img, final_boxes, final_scores, final_cls_inds,\n",
    "                         conf=0.3, class_names=COCO_CLASSES)\n",
    "    plt.figure(figsize = (200,20))\n",
    "    plt.axis('off')\n",
    "    plt.imshow(origin_img)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
